{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The AI Engineer Handbook is a practical, hands-on guide to AI Engineering. It aims to provide programmers a systematic introduction into building applications which use Large Language Models (LLMs) such as ChatGPT, GPT-4, Anthropic Claude and others. </p> <p>When providing code examples, care is taken to avoid the use of frameworks and instead providing code which demonstrates the high-level flow so that you can implement it yourself (or use a framework if you so choose).</p>"},{"location":"#pre-requisites","title":"Pre-requisites","text":"<p>This handbook is designed for programmers in mind, in particular programmers who can read/write Python. No prior machine learning or AI knowledge is assumed.</p>"},{"location":"#what-is-ai-engineering","title":"What is AI Engineering?","text":"<p>Accoring to @karpathy of OpenAI:</p> <p></p> <p>Key takeaways:</p> <ul> <li> <p>AI engineers use LLMs rather than build them</p> </li> <li> <p>They operate at a higher level of abstraction than ML engineers or LLM engineers</p> </li> <li> <p>AI engineers don't need to know how to build an LLM or a ML model, though it does help to know how they work</p> </li> <li> <p>There are some distinct skills which AI engineers do need to know such as:</p> <ul> <li> <p>Prompt engineering</p> </li> <li> <p>Working with data</p> </li> <li> <p>LLM infra</p> </li> <li> <p>Evaluating LLMs</p> </li> </ul> </li> </ul> <p>Swyx has more on this: The Rise of the AI Engineer</p>"},{"location":"#orientation","title":"Orientation","text":"<p>This handbook follows the Di\u00e1taxis documentation framework.</p> <p>If you are here to learn, go to tutorials. </p> <p>HOWTOs on the other hand, are task-oriented and exist to help you problem-solve and accomplish a task. </p> <p>The theory section is useful when you are looking to dive deeper into how things work.  </p> <p>The reference section provides a glossary.</p>"},{"location":"reference/glossary/","title":"Glossary of terms","text":"<ul> <li>Glossary of terms<ul> <li>Retrieval-augmented Generation</li> <li>Transformers</li> <li>Vector Embeddings</li> <li>Vector Search</li> </ul> </li> </ul>"},{"location":"reference/glossary/#retrieval-augmented-generation","title":"Retrieval-augmented Generation","text":"<p>Also known as RAG. </p> <p>Retrieval-Augmented Generation (RAG) combines information retrieval with large language models (LLMs) to  improve the factual accuracy and relevance of machine-generated text by accessing external databases.  This approach is particularly useful for generating responses to queries that require current knowledge or  specific details not contained within the model's pre-existing training data.</p> <p>At the core of RAG is the idea that while powerful, large language models (like GPT-3) are limited by the knowledge they were trained on, this limitation can be overcome by integrating a retrieval component, fetching relevant context at query-time  and populating the LLM prompt with said context. This retrieval step allows the model to access up-to-date and detailed information beyond what it has learned during its initial training.</p>"},{"location":"reference/glossary/#transformers","title":"Transformers","text":"<p>Transformers, in the context of Large Language Models (LLMs), are a type of neural network architecture that is particularly good at handling sequences of data, like text. Unlike previous models that processed data in order, transformers can look at all parts of the sequence at once. This allows them to understand context better and make connections between different parts of the data more effectively.</p> <p>The key innovation of transformers is the \"self-attention\" mechanism, which allows the model to weigh the importance of different parts of the input data when processing any single part. For example, when processing a sentence, the transformer can consider the context provided by the whole sentence to understand the meaning of each word better.</p> <p>Transformers are the foundation of many state-of-the-art LLMs, like GPT (Generative Pretrained Transformer) and BERT (Bidirectional Encoder Representations from Transformers), enabling them to generate coherent and contextually relevant text and perform a wide range of natural language understanding tasks.</p>"},{"location":"reference/glossary/#vector-embeddings","title":"Vector Embeddings","text":"<p>Vector embeddings are numerical representations of objects, such as words, sentences, images, or any kind of data, in a continuous vector space where similar items are mapped close to each other. These vectors are typically high-dimensional, meaning they may have hundreds or even thousands of elements, and they capture the essential qualities or features of the original items.</p> <p>Embeddings are created using machine learning algorithms, often neural networks, that are designed to learn the structure and relationships within a set of data. For example, in natural language processing, word embeddings represent words in such a way that words with similar meanings have similar vector representations. This allows for capturing the semantic meaning and relationships between words, beyond simple syntactic representation.</p> <p>The process of creating embeddings involves training a model on a dataset so that the model learns to assign vectors to each item in a way that reflects their similarities and differences. Once these embeddings are generated, they can be used for various tasks like similarity search, classification, clustering, and more. The key advantage of using vector embeddings is that they enable machines to understand and work with data in a more human-like, context-aware manner.</p>"},{"location":"reference/glossary/#vector-search","title":"Vector Search","text":"<p>Also known as Neural Search or Semantic Search. Vector search is a method of searching through a database or collection of items where each item is represented by a vector. A vector, in this context, is a list of numbers that captures the essential features of an item, whether it's a text document, an image, or any other type of data. asdsad</p> <p>Vector search is also called neural search because more recent vector search techniques use neural networks to create vector embeddings. </p>"},{"location":"theory/vector-search/","title":"Vector Search","text":"<p>TODO!</p>"},{"location":"tutorials/rag/","title":"Retrival-Augmented Generation (RAG) in 5 minutes","text":"<p>RAG, aka ChatGPT over private data, is the predominant way applications such as ChatPDF.com and the OpenAI Assistants API works. </p> <p>By the end of this tutorial, you will have built the simplest possible RAG application in 5 minutes from scratch without any frameworks.</p>"},{"location":"tutorials/rag/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval-Augmented Generation (RAG) combines information retrieval with large language models (LLMs) to  improve the factual accuracy and relevance of machine-generated text by accessing external databases.  This approach is particularly useful for generating responses to queries that require current knowledge or  specific details not contained within the model's pre-existing training data.</p> <p>At the core of RAG is the idea that while powerful, large language models (like GPT-3) are limited by the knowledge they were trained on, this limitation can be overcome by integrating a retrieval component, fetching relevant context at query-time  and populating the LLM prompt with said context. This retrieval step allows the model to access up-to-date and detailed information beyond what it has learned during its initial training.</p>"},{"location":"tutorials/rag/#why-rag","title":"Why RAG?","text":"<ul> <li>Up-to-Date Information: Static models may not always have access to the latest information. RAG ensures that the   generated content can be augmented with the most recent data.</li> <li>Private data: Sometimes, the context required to answer a question or complete a prompt is not found   within   the model's pre-existing knowledge. RAG can pull in the necessary context from external sources.</li> <li>Hallucination mitigation The generated answer is grounded in data from the retrieval phase, reducing the likelihood of hallucination.</li> <li>Resource Efficiency: Fine-tuning large language models with new information is computationally expensive and   time-consuming. RAG offers a more efficient alternative by retrieving information on-the-fly.</li> </ul>"},{"location":"tutorials/rag/#the-application","title":"The application","text":"<p>We want to build a Python application that lets us ask ChatGPT questions over our knowledge base (containing 2 documents) and get back answers that are grounded in these 2 documents (and not from ChatGPT's entire knowledge base).</p> <p>The knowledge base is: </p> <ol> <li> <p>Horses are domesticated mammals known for their strength, speed, and versatility. They have been crucial to human civilization for transportation, agriculture, and recreational activities. Horses belong to the Equidae family and are herbivores with a digestive system adapted to grazing. Common breeds include the Arabian, Thoroughbred, and Clydesdale.</p> </li> <li> <p>Zebras are African equids known for their distinctive black and white striped coat patterns. They belong to the genus Equus, which also includes horses and donkeys. Zebras are herbivores and primarily graze on grasses. </p> </li> </ol> <p>Note</p> <p>In reality, if the knowledge base only comprised these 2 documents, we wouldn't bother to RAG as these are small enough to fit into the context. However, we're using this simple case to demonstrate how RAG works in general. </p>"},{"location":"tutorials/rag/#install-dependencies","title":"Install dependencies","text":"<pre><code>pip install openai chromadb\n</code></pre> <p>Sign-up for an OpenAI API key if you haven't already.</p> <p>ChromaDB is the vector database we're using. </p>"},{"location":"tutorials/rag/#initialization","title":"Initialization","text":"<pre><code>import chromadb\nimport chromadb.utils.embedding_functions as embedding_functions\nfrom openai import OpenAI\n\nOPENAI_API_KEY = \"sk-xxx\" # CHANGEME!\nchroma_collection = \"animals\"\nembedding_function = embedding_functions.OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY)\nchroma_client = chromadb.PersistentClient(path=\"vector_store\")\nopenai_client = OpenAI(api_key=OPENAI_API_KEY)\n</code></pre>"},{"location":"tutorials/rag/#vector-store-indexing","title":"Vector store indexing","text":"<p>Given a question, we need to retrieve relevant documents that are then populated into the LLM prompt. </p> <p>We'll be using a local vector database (Chroma) and OpenAI vector embeddings to perform this retrieval phase, but the general method applies irrespective of the choice of vector database or embeddings. </p> <p>Note</p> <p>It is important to use the exact same vector embeddings when indexing and retrieving documents from the vector database.</p> <p>Before retrieving the documents, we first have to add them to Chroma.</p> <pre><code>def add_docs_to_vector_store(documents: list[str],\n                             metadatas: list[dict] = None, ids: list[str] = None,\n                             ):\n    collection: Collection = chroma_client.get_or_create_collection(name=chroma_collection,\n                                                                    embedding_function=embedding_function)\n    if ids is None:\n        ids = [str(counter) for counter, doc in enumerate(documents, 1)]\n    collection.add(\n        documents=documents,\n        metadatas=metadatas,\n        ids=ids\n    )\n</code></pre>"},{"location":"tutorials/rag/#the-rag-step-proper","title":"The RAG step proper","text":"<p>The RAG step involves retrieving matching documents from Chroma, then populating the prompt with the retrieved documents.</p> <pre><code>def rag(query: str):\n    collection: Collection = chroma_client.get_or_create_collection(name=chroma_collection,\n                                                                    embedding_function=embedding_function)\n    docs = collection.query(query_texts=query, n_results=5)\n    prompt = retrieval_prompt(docs[\"documents\"][0], query)\n    return llm_call(prompt, \"You are a helpful AI assistant.\", model=\"gpt-3.5-turbo\", temperature=0)\n\ndef retrieval_prompt(docs, query) -&gt; str:\n    return f\"\"\"Answer the QUESTION using only the CONTEXT given, nothing else. \nDo not make up an answer, say 'I don't know' if you are not sure. Be succinct.\nQUESTION: {query}\nCONTEXT: {[doc for doc in docs]}\nANSWER:\n\"\"\"\n</code></pre> <p>The retrieval prompt instructs ChatGPT to answer based on the provided context.</p> <p>The <code>llm_call()</code> function is a utility method that calls the completion OpenAI endpoint.</p> <pre><code>def llm_call(prompt, system_prompt=\"\", **kwargs) -&gt; str:\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n    kwargs[\"messages\"] = messages\n    response = openai_client.chat.completions.create(**kwargs)\n    generated_texts = [\n        choice.message.content.strip() for choice in response.choices\n    ]\n    return \" \".join(generated_texts)\n</code></pre>"},{"location":"tutorials/rag/#the-main-function","title":"The main() function","text":"<pre><code>if __name__ == \"__main__\":\n    add_docs_to_vector_store([\n        \"Horses are domesticated mammals known for their strength, speed, and versatility. They have been crucial to human civilization for transportation, agriculture, and recreational activities. Horses belong to the Equidae family and are herbivores with a digestive system adapted to grazing. Common breeds include the Arabian, Thoroughbred, and Clydesdale.\",\n        \"Zebras are African equids known for their distinctive black and white striped coat patterns. They belong to the genus Equus, which also includes horses and donkeys. Zebras are herbivores and primarily graze on grasses. \"])\n    print(rag(\"What genus do horses belong to?\"))\n    print(rag(\"What genus do zebras belong to?\"))\n    print(rag(\"Are zebras and horses of the same genus?\"))\n    print(rag(\"What genus do snakes belong to?\"))\n\n# Horses belong to the genus Equus.\n# Zebras belong to the genus Equus.\n# Yes.\n# I don't know.    \n</code></pre>"},{"location":"tutorials/rag/#the-entire-file","title":"The entire file","text":"<pre><code>import chromadb\nimport chromadb.utils.embedding_functions as embedding_functions\nfrom chromadb.api.models import Collection\nfrom openai import OpenAI\n\nOPENAI_API_KEY = \"sk-xxx\" # CHANGEME\nchroma_collection = \"animals\"\nembedding_function = embedding_functions.OpenAIEmbeddingFunction(api_key=OPENAI_API_KEY)\nchroma_client = chromadb.PersistentClient(path=\"vector_store\")\nopenai_client = OpenAI(api_key=OPENAI_API_KEY)\n\n\ndef add_docs_to_vector_store(documents: list[str],\n                             metadatas: list[dict] = None, ids: list[str] = None,\n                             ):\n    collection: Collection = chroma_client.get_or_create_collection(name=chroma_collection,\n                                                                    embedding_function=embedding_function)\n    if ids is None:\n        ids = [str(counter) for counter, doc in enumerate(documents, 1)]\n    collection.add(\n        documents=documents,\n        metadatas=metadatas,\n        ids=ids\n    )\n\n\ndef retrieval_prompt(docs, query) -&gt; str:\n    return f\"\"\"Answer the QUESTION using only the CONTEXT given, nothing else. \nDo not make up an answer, say 'I don't know' if you are not sure. Be succinct.\nQUESTION: {query}\nCONTEXT: {[doc for doc in docs]}\nANSWER:\n\"\"\"\n\n\ndef rag(query: str):\n    collection: Collection = chroma_client.get_or_create_collection(name=chroma_collection,\n                                                                    embedding_function=embedding_function)\n    docs = collection.query(query_texts=query, n_results=2)\n    prompt = retrieval_prompt(docs[\"documents\"][0], query)\n    return llm_call(prompt, \"You are a helpful AI assistant.\", model=\"gpt-3.5-turbo\", temperature=0.0)\n\n\ndef llm_call(prompt, system_prompt=\"\", **kwargs) -&gt; str:\n    messages = [{\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": prompt}]\n    kwargs[\"messages\"] = messages\n    response = openai_client.chat.completions.create(**kwargs)\n    generated_texts = [\n        choice.message.content.strip() for choice in response.choices\n    ]\n    return \" \".join(generated_texts)\n\n\nif __name__ == \"__main__\":\n    add_docs_to_vector_store([\n        \"Horses are domesticated mammals known for their strength, speed, and versatility. They have been crucial to human civilization for transportation, agriculture, and recreational activities. Horses belong to the Equidae family and are herbivores with a digestive system adapted to grazing. Common breeds include the Arabian, Thoroughbred, and Clydesdale.\",\n        \"Zebras are African equids known for their distinctive black and white striped coat patterns. They belong to the genus Equus, which also includes horses and donkeys. Zebras are herbivores and primarily graze on grasses. \"])\n    print(rag(\"What genus do horses belong to?\"))\n    print(rag(\"What genus do zebras belong to?\"))\n    print(rag(\"Are zebras and horses of the same genus?\"))\n    print(rag(\"What genus do snakes belong to?\"))\n</code></pre>"},{"location":"tutorials/rag/#where-to-from-here","title":"Where to from here?","text":"<p>Congratulations! You've built your first RAG application. </p> <p>For the sake of brevity, conciseness and ease of comprehension, we've omitted all kinds of important stuff  that would make this code more production-ready, such as:</p> <ol> <li>Storing the OPENAI_API_KEY into a .env file and using <code>python-dotenv</code> to load it.</li> <li>Implementing a backoff mechanism when calling OpenAI, e.g. using tenacity or backoff. </li> <li>Implementing chunking before vector store indexing to handle larger documents.</li> <li>Implementing text extraction/parsing to handle PDF, Word, epub files etc.</li> </ol> <p>Additionally, some things that might be a good to experiment with include:</p> <ol> <li>Instead of using an incrementing counter, using a hashing/fingerprinting function (e.g. <code>farmhash</code>) to create content hashes.</li> <li>Experiment with using other embedding models other than OpenAI's embeddings.</li> </ol>"}]}